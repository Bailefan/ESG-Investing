{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to reproduce paper results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply start executing cells. You can play around with different parameters if you wish so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import accumulate\n",
    "from dtaidistance import dtw_visualisation as dtwvis\n",
    "from company_info import company_info_list\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from esg_dtw import filter_data, get_daily_estimator, get_price_data, apply_dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-01-01'\n",
    "# start_date = '2024-01-01'\n",
    "# end_date = '2024-02-29'\n",
    "# end_date = '2023-12-31'\n",
    "end_date = '2024-09-17'\n",
    "\n",
    "sent_dict = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "\n",
    "# LOAD DATA\n",
    "data = pd.read_json('nano_esg_2024_09_17.json', lines=True)\n",
    "\n",
    "\n",
    "relevance_cutoff = 0 #take relevance scores bigger than the cutoff. Not used in the paper - instead we use the relevance score as a weight\n",
    "# All aspect-combinations. None does not filter by aspect, meaning it includes all aspects\n",
    "aspect_filters = [None, 'environmental', 'social', 'governance', 'env_soc', 'env_gov', 'soc_gov']\n",
    "\n",
    "# Weight each article by its relevance score\n",
    "weighted_relevance = True\n",
    "weighted_relevance_dict = {1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10}\n",
    "\n",
    "dtw_window = 5\n",
    "\n",
    "data['sentiment_int'] = data['sentiment'].apply(lambda x: sent_dict[x])\n",
    "data['original_date'] = data['date'].copy()\n",
    "\n",
    "#Shift date one day forward if the hour of release is after 17:00\n",
    "def add_day_if_late(timestamp, cutoff_hour=17):\n",
    "    if timestamp.hour > cutoff_hour:\n",
    "        timestamp += pd.Timedelta(days=1)\n",
    "    return timestamp\n",
    "data['date'] = data['date'].apply(add_day_if_late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, only perform calculations for a single company (in the paper, we use 'daimler' as an example)\n",
    "selected_company = None\n",
    "\n",
    "# Get the price data and ESG data per aspect for each company\n",
    "company_aspect_daily_data_dict = {}\n",
    "company_aspect_esg_sent_dict = {}\n",
    "company_price_dict = {}\n",
    "company_date_dt_dict = {}\n",
    "for company in tqdm(company_info_list.keys(), desc='Gathering data...'):\n",
    "    if selected_company and company != selected_company:\n",
    "        continue\n",
    "    company_aspect_daily_data_dict[company] = {}\n",
    "    company_aspect_esg_sent_dict[company] = {}\n",
    "    company_date_dt_dict[company] = {}\n",
    "\n",
    "    #Get stock data\n",
    "    df = get_price_data(company, start_date = start_date, end_date = end_date)\n",
    "    company_price_dict[company] = df['Price'].to_list()\n",
    "    company_date_dt_dict[company] = df.index.to_list()\n",
    "    for aspect_filter in aspect_filters:\n",
    "        daily_data = filter_data(data, start_date, end_date, company, aspect_filter, relevance_cutoff, weighted_relevance)\n",
    "        daily_data.fillna(0, inplace=True)\n",
    "\n",
    "        #get daily sentiment\n",
    "        sentiment_weekdays = []\n",
    "        for index, row in daily_data.iterrows():\n",
    "            if index in df.index.to_list():\n",
    "                sentiment_weekdays.append(row['sentiment_int'])\n",
    "        esg_sentiment = list(accumulate(sentiment_weekdays))\n",
    "\n",
    "        company_aspect_daily_data_dict[company][aspect_filter] = daily_data\n",
    "        company_aspect_esg_sent_dict[company][aspect_filter] = esg_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we iterate over the time series and perform DTW at each point to determine the best aspect to use for the estimator without using future information\n",
    "# At the same time, we keep track of multiple variables which we will investigate in the plots below\n",
    "\n",
    "mode=''              #General or ''. If General, we only consider the None aspect filter (all aspects combined)\n",
    "use_lag_to_invest = True\n",
    "data_window = 70         #days - set to None to simply take everything before this point\n",
    "\n",
    "company_estimator_dict = {}         #here, we always just append the currently decided estimator\n",
    "company_shifted_estimator_dict = {} #here, we always replace the last x entries of the estimator (since we take a data_window to make this decision)\n",
    "company_best_aspect_dict = {}\n",
    "distance_dict = {}\n",
    "dtw_dict_per_comp = {}  #for paper\n",
    "for company in tqdm(company_info_list.keys(), desc='Calculating daily return estimators...'):\n",
    "    if selected_company and company != selected_company:\n",
    "        continue\n",
    "    company_estimator_dict[company] = [0, 0]\n",
    "    company_shifted_estimator_dict[company] = [0, 0]\n",
    "    company_best_aspect_dict[company] = [None, None]\n",
    "    distance_dict[company] = [0, 0]\n",
    "    dtw_dict_per_comp[company] = [{}, {}]\n",
    "    #Iterate over days, determine best aspect and set the return estimator\n",
    "    for i in range(2, len(company_date_dt_dict[company])):\n",
    "        aspect_distance_dict_True = {}  #Keeps track of the actual distances, not manipulated by lag\n",
    "        aspect_distance_dict = {}       #Keeps track of the distances, manipulated by lag (set to 1000 if lag is negative)\n",
    "        aspect_lag_dict = {}            #Keeps track of the lag of the aspects\n",
    "        no_trade_dict = {}              #Keeps track of whether we are allowed to trade this aspect or not\n",
    "        dtw_dict = {}\n",
    "        for aspect_filter in aspect_filters:\n",
    "            dtw_dict[aspect_filter] = {}    #temp\n",
    "            no_trade_dict[aspect_filter] = False\n",
    "            #if mode is general, retrieve only the None aspect filter\n",
    "            if mode == 'General' and aspect_filter is not None:\n",
    "                continue\n",
    "            #retrieve data from dicts - including that day\n",
    "            if data_window:\n",
    "                price = company_price_dict[company][np.max([0, i-data_window+1]):i+1]\n",
    "                esg_sentiment = company_aspect_esg_sent_dict[company][aspect_filter][np.max([0, i-data_window+1]):i+1]\n",
    "            else:\n",
    "                price = company_price_dict[company][:i+1]\n",
    "                esg_sentiment = company_aspect_esg_sent_dict[company][aspect_filter][:i+1]\n",
    "            #Normalize s1, s2, get sentiment gradient and calculate best path\n",
    "            s1, s2, d, best_path = apply_dtw(price, esg_sentiment, int(np.min([len(esg_sentiment), dtw_window])))\n",
    "\n",
    "            dtw_dict[aspect_filter]['d'] = d\n",
    "            dtw_dict[aspect_filter]['best_path'] = best_path\n",
    "            dtw_dict[aspect_filter]['price'] = s1\n",
    "            dtw_dict[aspect_filter]['sentiment'] = s2\n",
    "\n",
    "            aspect_distance_dict_True[aspect_filter] = d\n",
    "            #Calculate lag - set distance to 1000 if it is negative\n",
    "            lag = [l[0] - l[1] for l in best_path]\n",
    "            if lag:\n",
    "                if lag[-1] <= 0 and use_lag_to_invest:\n",
    "                    #price is leading esg-sentiment -> we cannot use this to predict future returns\n",
    "                    no_trade_dict[aspect_filter] = True     #this is not enough because there could still be an aspect which has pos lag but slightly\n",
    "                                                            #worse distance which would not be picked without manipulating the distance\n",
    "                    d = 1000\n",
    "            aspect_distance_dict[aspect_filter] = d\n",
    "            aspect_lag_dict[aspect_filter] = lag[-1] if lag else -10\n",
    "            \n",
    "        dtw_dict_per_comp[company].append(dtw_dict)\n",
    "        \n",
    "        #Get aspect with smallest distance\n",
    "        best_aspect = min(aspect_distance_dict, key=aspect_distance_dict.get)\n",
    "\n",
    "        distance_dict[company].append(aspect_distance_dict_True[best_aspect])\n",
    "\n",
    "\n",
    "        #If the smallest distance is still too big, set the estimator to be neutral\n",
    "        if aspect_distance_dict[best_aspect] >= 1000 or no_trade_dict[best_aspect] and (mode != 'General' and use_lag_to_invest is not False):\n",
    "            company_estimator_dict[company].append(0)\n",
    "            best_aspect = None\n",
    "        else:\n",
    "            company_estimator_dict[company].append(company_aspect_daily_data_dict[company][best_aspect].loc[company_date_dt_dict[company][i]].values[0])\n",
    "        \n",
    "        #Gather variables\n",
    "        company_best_aspect_dict[company].append(best_aspect)\n",
    "        company_shifted_estimator_dict[company].append(company_aspect_daily_data_dict[company][best_aspect].loc[company_date_dt_dict[company][i]].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Combined ESG-Sentiment Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create this graph by always taking the sentiment of the determined aspect-combination above. Then, we need to re-normalize the resulting graph using apply_dtw(), so that the price and sentiment is on the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_company_dict = {}\n",
    "for comp in tqdm(company_info_list.keys(), desc='Retrieving Composed ESG Sentiment Series...'):\n",
    "    \n",
    "    price = company_price_dict[comp]\n",
    "    composed_esg = company_shifted_estimator_dict[comp]\n",
    "    composed_esg_acc = list(accumulate(composed_esg))\n",
    "    s1, s2, d, best_path = apply_dtw(price, composed_esg_acc, dtw_window)\n",
    "    composed_company_dict[comp] = {'price': s1, 'esg': s2, 'distance': d, 'best_path': best_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HP Optimization - This creates .csv files with the estimators for every HP combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide all the estimator .csv files for the HP Optimization we present in the paper under the folder _hp_opt_. If you want to reproduce them, be aware that the different source of price data (yfinance) can lead to different estimators. However, it should be very similar.\n",
    "\n",
    "We do not provide a way to do portfolio optimization in this notebook, but if you have a pipeline set up you can use the files and test it on your own. Be sure to adjust the file_name according to the path you want to save the .csv files at.\n",
    "\n",
    "Note: executing this for a large number of combinations can take a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this code to create a set of estimator files for optimizing the hyperparameters\n",
    "\n",
    "dtw_window_list = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15]\n",
    "data_window_list = [20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "use_lag_to_invest = True\n",
    "daily_est_kwargs = {\n",
    "    'company_date_dt_dict': company_date_dt_dict,\n",
    "    'company_price_dict': company_price_dict,\n",
    "    'company_aspect_esg_sent_dict': company_aspect_esg_sent_dict,\n",
    "    'company_aspect_daily_data_dict': company_aspect_daily_data_dict,\n",
    "    'aspect_filters': aspect_filters,\n",
    "    'use_lag_to_invest': use_lag_to_invest,\n",
    "}\n",
    "\n",
    "for dtw_window in dtw_window_list:\n",
    "    for data_window in data_window_list:\n",
    "        print(f'\\rCalculating for {dtw_window} DTW window and {data_window} data window...', end='')\n",
    "        company_estimator_dict = get_daily_estimator(dtw_window, data_window, daily_est_kwargs)\n",
    "\n",
    "        #Create dataframe\n",
    "        company_ret_est_df = pd.DataFrame()\n",
    "        for company in company_info_list.keys():\n",
    "            company_df = pd.DataFrame(company_estimator_dict[company])\n",
    "            company_df.columns = [company_info_list[company]['db_name']]\n",
    "            company_df.index = company_date_dt_dict[company]\n",
    "            company_df.index.names = ['Date']\n",
    "            if len(company_ret_est_df) == 0:\n",
    "                company_ret_est_df = company_df\n",
    "            else:\n",
    "                company_ret_est_df = company_ret_est_df.merge(company_df, on='Date', how='outer')\n",
    "\n",
    "        file_name = f'estimator_files/hp_opt/esg_sentiments_{data_window}DW_{dtw_window}dtw.csv'\n",
    "        company_ret_est_df.to_csv(file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regime Ratios - Table 3 in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_lengths_comp = {}\n",
    "regime_lengths_sum = {}\n",
    "for comp in company_best_aspect_dict:\n",
    "    \n",
    "    occurences = Counter(company_best_aspect_dict[comp])\n",
    "\n",
    "    regime_lengths_comp[comp] = {i: occurences[i] for i in occurences if i}\n",
    "    regime_lengths_sum[comp] = sum([occurences[i] for i in occurences if i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look specifically at auto industry\n",
    "auto_industry = ['daimler', 'bmw', 'vw', 'porsche_ag']\n",
    "finance_industry = ['deutsche_bank', 'commerzbank', 'deutsche_boerse']\n",
    "insurance_industry = ['allianz', 'munich_re', 'hannover_re']\n",
    "chemical_industry = ['basf', 'bayer', 'covestro', 'brenntag']\n",
    "consumer_industry = ['beiersdorf', 'henkel', 'adidas', 'zalando']\n",
    "infrastructure_industry = ['rwe', 'siemens_energy', 'eon']\n",
    "\n",
    "# Get regime lengths for all companies\n",
    "all_comps = [comp for comp in company_best_aspect_dict]\n",
    "regime_lengths_all = {i: 0 for i in aspect_filters if i}\n",
    "for comp in all_comps:\n",
    "    for aspect in regime_lengths_all:\n",
    "        if aspect in regime_lengths_comp[comp]:\n",
    "            regime_lengths_all[aspect] += regime_lengths_comp[comp][aspect]\n",
    "all_companies = {k: v/np.sum(list(regime_lengths_all.values())) for k, v in regime_lengths_all.items()}\n",
    "\n",
    "# Get regime lengths for a custom set of companies / industries\n",
    "# Vary the custom industry list to reproduce the different rows in the Table\n",
    "custom_industry = auto_industry\n",
    "\n",
    "regime_lengths = {i: 0 for i in aspect_filters if i}\n",
    "for comp in custom_industry:\n",
    "    for aspect in regime_lengths:\n",
    "        if aspect in regime_lengths_comp[comp]:\n",
    "            regime_lengths[aspect] += regime_lengths_comp[comp][aspect]\n",
    "\n",
    "industry_companies = {k: v/np.sum(list(regime_lengths.values())) for k, v in regime_lengths.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that because of the different datasource for the prices (yfinance), the results are slightly different from the ones in the paper.\n",
    "\n",
    "However, this does not take away from the derived conclusions presented there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for asp in all_companies:\n",
    "    print(asp, (industry_companies[asp]-all_companies[asp])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting keys and values together to ensure the correct order\n",
    "label_order = ['environmental', 'env_soc', 'social', 'soc_gov', 'governance', 'env_gov']\n",
    "\n",
    "labels = label_order\n",
    "sizes = [regime_lengths_all[i] for i in label_order]\n",
    "\n",
    "# Creating the pie chart\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Equal aspect ratio ensures the pie is drawn as a circle\n",
    "plt.axis('equal')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate a single company - in the paper, we use 'daimler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = 'daimler'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, clean the regimes so that fluctuations of a few days dont disrupt them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamap = {'All': 2, 'environmental': 3, 'env_soc': 4, 'social': 5, 'soc_gov': 6, 'governance': 7, 'env_gov': 8}\n",
    "for i in range(0 ,len(company_best_aspect_dict[comp])):\n",
    "    if company_best_aspect_dict[comp][i] is None:\n",
    "        company_best_aspect_dict[comp][i] = 'All'\n",
    "dataplot = list(map(lambda x: datamap[x], company_best_aspect_dict[comp]))\n",
    "\n",
    "#Often, regimes fluctuate a bit. For display purposes, we disregard regime fluctuations that are shorter than a certain amount of days\n",
    "regime_cutoff = 5 #days for an aspect series to build a regime\n",
    "fluctuation_cutoff = 2  #If the aspect changes for more than this amount, we start a new regime\n",
    "\n",
    "false_regimes = ['no_correlation', None]\n",
    "\n",
    "new_regime_status = False\n",
    "new_regime = []\n",
    "regimes = []\n",
    "regimes_aspects = []\n",
    "aspect_regime = []\n",
    "curr_reg_aspect = 0\n",
    "new_reg_aspect = 0\n",
    "for i in range(1, len(company_best_aspect_dict[comp])):\n",
    "    #If we detect a change in aspect, we start a new regime\n",
    "    if company_best_aspect_dict[comp][i] != company_best_aspect_dict[comp][i-1]:\n",
    "        new_reg_aspect = company_best_aspect_dict[comp][i]\n",
    "        new_regime_status = True\n",
    "        if new_reg_aspect == curr_reg_aspect:\n",
    "            #If the new regime was a short fluctuation, we continue with the old (current) regime\n",
    "            new_regime_status = False\n",
    "            aspect_regime = aspect_regime + new_regime\n",
    "            aspect_regime.append(company_date_dt_dict[comp][i])\n",
    "        else:\n",
    "            new_regime = []\n",
    "    elif company_best_aspect_dict[comp][i] not in false_regimes and not new_regime_status:\n",
    "        curr_reg_aspect = company_best_aspect_dict[comp][i]\n",
    "        aspect_regime.append(company_date_dt_dict[comp][i])\n",
    "    if len(new_regime) >= fluctuation_cutoff and new_regime_status:\n",
    "        #If the new regime is not a fluctuation, we add the old regime to the list\n",
    "        if len(aspect_regime) > regime_cutoff and curr_reg_aspect != 'no_correlation':\n",
    "            regimes.append(aspect_regime)\n",
    "            regimes_aspects.append(curr_reg_aspect)\n",
    "        new_regime_status = False\n",
    "        if new_reg_aspect not in false_regimes:\n",
    "            #If the new regime is not a no_correlation regime, we continue with it as the current regime\n",
    "            aspect_regime = new_regime.copy()\n",
    "            curr_reg_aspect = company_best_aspect_dict[comp][i]\n",
    "            new_regime = []\n",
    "        else:\n",
    "            #otherwise we stop with the new regime\n",
    "            curr_reg_aspect = 'no_correlation'\n",
    "            new_regime = []\n",
    "    elif new_regime_status:\n",
    "        new_regime.append(company_date_dt_dict[comp][i])\n",
    "#if we reach the end of data while in a regime, we add it to the list\n",
    "if curr_reg_aspect not in false_regimes and len(aspect_regime) > regime_cutoff:\n",
    "    regimes.append(aspect_regime)\n",
    "    regimes_aspects.append(curr_reg_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_before = int(data_window)\n",
    "\n",
    "data_comp = data[data['company'] == comp]\n",
    "data_comp = data_comp[data_comp['relevance_score'] >= relevance_cutoff]\n",
    "regime_info = {}\n",
    "distances = []\n",
    "for r in range(0, len(regimes)):\n",
    "    start_idx = company_date_dt_dict[comp].index(regimes[r][0])\n",
    "    end_idx = company_date_dt_dict[comp].index(regimes[r][-1])\n",
    "    if regimes_aspects[r] is None:\n",
    "        data_comp_asp = data_comp.copy()\n",
    "    elif regimes_aspects[r] == 'env_soc':\n",
    "        data_comp_asp = data_comp[data_comp['aspect'] != 'governance']\n",
    "    elif regimes_aspects[r] == 'env_gov':\n",
    "        data_comp_asp = data_comp[data_comp['aspect'] != 'social']\n",
    "    elif regimes_aspects[r] == 'soc_gov':\n",
    "        data_comp_asp = data_comp[data_comp['aspect'] != 'environmental']\n",
    "    else:\n",
    "        data_comp_asp = data_comp[data_comp['aspect'] == regimes_aspects[r]]\n",
    "    data_r = data_comp_asp[(data_comp_asp['date'] >= pd.Timestamp(regimes[r][0])) & (data_comp_asp['date'] <= pd.Timestamp(regimes[r][-1]))]\n",
    "    if weighted_relevance:\n",
    "        data_r['weighted_relevance'] = data_r['relevance_score'].apply(lambda x: weighted_relevance_dict[x])\n",
    "\n",
    "    regime_price = company_price_dict[comp][np.max([0, start_idx-time_before]):end_idx+1]\n",
    "    regime_esg = company_shifted_estimator_dict[comp][np.max([0, start_idx-time_before]):end_idx+1]\n",
    "    regime_esg_acc = list(accumulate(regime_esg))\n",
    "    s1, s2, d, best_path = apply_dtw(regime_price, regime_esg_acc, dtw_window)\n",
    "    distances.append(d/len(best_path))\n",
    "\n",
    "    start_price = company_price_dict[comp][start_idx]\n",
    "    end_price = company_price_dict[comp][end_idx]\n",
    "\n",
    "    regime_info[r] = {'start_idx': start_idx,\n",
    "                      'end_idx': end_idx,\n",
    "                      'regime_aspect': regimes_aspects[r],\n",
    "                      'articles': data_r,\n",
    "                      'returns': (end_price - start_price) / start_price,\n",
    "                      'price': s1,\n",
    "                      'esg': s2,\n",
    "                      'distance': d,\n",
    "                      'best_path': best_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create clean regime array\n",
    "clean_regimes = [None] * len(company_best_aspect_dict[comp])\n",
    "for reg in regime_info:\n",
    "    clean_regimes[regime_info[reg]['start_idx']:regime_info[reg]['end_idx']+1] = [regime_info[reg]['regime_aspect']] * (regime_info[reg]['end_idx'] - regime_info[reg]['start_idx'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0 ,len(clean_regimes)):\n",
    "    if clean_regimes[i] is None:\n",
    "        clean_regimes[i] = 'All'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 of paper in the three cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, investigate the 'cleaned' regimes (this plot is not included in the paper)\n",
    "\n",
    "datamap = {'All': 2, 'environmental': 3, 'env_soc': 4, 'social': 5, 'soc_gov': 6, 'governance': 7, 'env_gov': 8}\n",
    "dataplot = list(map(lambda x: datamap[x], clean_regimes))\n",
    "\n",
    "fig_esg, ax1 = plt.subplots()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(company_date_dt_dict[comp], composed_company_dict[comp]['price'], label='price')\n",
    "ax2.plot(company_date_dt_dict[comp], composed_company_dict[comp]['esg'], label='esg')\n",
    "ax1.plot(company_date_dt_dict[comp], dataplot, label='best aspect', color='red')\n",
    "\n",
    "\n",
    "r = 12\n",
    "start_idx = np.max([0, regime_info[r]['start_idx']])\n",
    "end_idx = regime_info[r]['end_idx']\n",
    "ax2.axvspan(company_date_dt_dict[comp][start_idx], company_date_dt_dict[comp][end_idx], color='green', alpha=0.1)\n",
    "ax1.set_yticks([2,3,4,5,6,7,8], list(datamap.keys()))\n",
    "ax2.legend()\n",
    "ax1.set_title(comp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After a regime has been highlighted above, we can investigate its warping path below\n",
    "\n",
    "idx = r\n",
    "print(regime_info[idx]['regime_aspect'])\n",
    "reg_start_idx = np.max([0, regime_info[idx]['start_idx'] - np.max([0, regime_info[idx]['start_idx']-time_before])])\n",
    "reg_end_idx = np.max([0, regime_info[idx]['end_idx'] - np.max([0, regime_info[idx]['start_idx']-time_before])])\n",
    "fig_wp, axes = dtwvis.plot_warping(regime_info[idx]['price'], regime_info[idx]['esg'], regime_info[idx]['best_path'])\n",
    "\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[1].set_ylabel('ESG')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].axvspan(reg_start_idx, reg_end_idx, color='green', alpha=0.3)\n",
    "axes[0].axvspan(reg_start_idx, reg_end_idx, color='green', alpha=0.3)\n",
    "\n",
    "for item in ([axes[0].title, axes[0].xaxis.label, axes[0].yaxis.label] +\n",
    "             axes[0].get_xticklabels() + axes[0].get_yticklabels()\n",
    "             + [axes[1].title, axes[1].xaxis.label, axes[1].yaxis.label] +\n",
    "             axes[1].get_xticklabels() + axes[1].get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "for ax in axes:\n",
    "    # Get all the Line2D objects in the axis and change their linewidth\n",
    "    for line in ax.get_lines():\n",
    "        line.set_linewidth(3)  # Set your desired linewidth here\n",
    "\n",
    "print(regime_info[idx]['distance'])\n",
    "print(f'Final Lag: {regime_info[idx][\"best_path\"][-1][0] - regime_info[idx][\"best_path\"][-1][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we display the original aspect-regimes with the detected regime highlighted\n",
    "datamap = {'All': 2, 'environmental': 3, 'env_soc': 4, 'social': 5, 'soc_gov': 6, 'governance': 7, 'env_gov': 8}\n",
    "for i in range(0 ,len(company_best_aspect_dict[comp])):\n",
    "    if company_best_aspect_dict[comp][i] is None:\n",
    "        company_best_aspect_dict[comp][i] = 'All'\n",
    "dataplot = list(map(lambda x: datamap[x], company_best_aspect_dict[comp]))\n",
    "\n",
    "fig_esg, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_title('Daimler - Aspect-Regimes')\n",
    "ax1.set_ylabel('Aspect')\n",
    "ax1.set_xlabel('Date')\n",
    "ax2.set_ylabel('Normalized Price/Sentiment')\n",
    "\n",
    "for item in ([ax1.title, ax1.xaxis.label, ax1.yaxis.label] +\n",
    "             ax1.get_xticklabels() + ax1.get_yticklabels() +\n",
    "             [ax2.title, ax2.xaxis.label, ax2.yaxis.label] +\n",
    "             ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "    item.set_fontsize(10)\n",
    "\n",
    "ax1.plot(company_date_dt_dict[comp], dataplot, label='best aspect', color='red', alpha=0.7)\n",
    "ax2.plot(company_date_dt_dict[comp], composed_company_dict[comp]['price'], label='price', linewidth=2)\n",
    "ax2.plot(company_date_dt_dict[comp], composed_company_dict[comp]['esg'], label='esg', linewidth=2)\n",
    "\n",
    "#Find regimes below first\n",
    "ax2.axvspan(company_date_dt_dict[comp][start_idx], company_date_dt_dict[comp][end_idx], color='green', alpha=0.3)\n",
    "\n",
    "\n",
    "ax1.set_yticks([2,3,4,5,6,7,8], list(datamap.keys()))\n",
    "ax1.tick_params(labelrotation=45)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce the plots used in Figure 1 of the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_indx = 391\n",
    "total_price = company_price_dict[comp][:end_indx]\n",
    "time = company_date_dt_dict[comp][:end_indx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_price, ax = plt.subplots()\n",
    "plt.plot(time, total_price, linewidth=3)\n",
    "plt.title(f'Company: {comp}')\n",
    "plt.xlabel('Time')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Price')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax.axvspan(time[len(time)-70], time[-1], color='green', alpha=0.3)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary the aspect to reproduce the 6 plots (3 esg sentiments & 3 warping paths) of the schema!\n",
    "\n",
    "plt.style.use(['default'])\n",
    "aspect_filter = 'governance'\n",
    "total_aspect_sentiment = company_aspect_esg_sent_dict[comp][aspect_filter][:end_indx]\n",
    "\n",
    "fig_asp, ax = plt.subplots()\n",
    "plt.plot(time, total_aspect_sentiment, linewidth=3)\n",
    "plt.title(f'Company: {comp}')\n",
    "plt.xlabel('Time')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Soc-Gov Sentiment')\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "ax.axvspan(time[len(time)-70], time[-1], color='green', alpha=0.3)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_filter = 'environmental'\n",
    "dtw_idx = end_indx\n",
    "\n",
    "d = dtw_dict_per_comp[comp][dtw_idx][aspect_filter]['d']\n",
    "best_path = dtw_dict_per_comp[comp][dtw_idx][aspect_filter]['best_path']\n",
    "lag = [l[0] - l[1] for l in best_path]\n",
    "price = dtw_dict_per_comp[comp][dtw_idx][aspect_filter]['price']\n",
    "sentiment = dtw_dict_per_comp[comp][dtw_idx][aspect_filter]['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in aspect_filters:\n",
    "    print(i, dtw_dict_per_comp[comp][dtw_idx][i]['d'])\n",
    "    print(i, dtw_dict_per_comp[comp][dtw_idx][i]['best_path'][-1][0] - dtw_dict_per_comp[comp][dtw_idx][i]['best_path'][-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dtw, axes = dtwvis.plot_warping(price, sentiment, best_path)\n",
    "\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[1].set_xlabel('Days')\n",
    "axes[1].set_ylabel('Sentiment')\n",
    "for item in ([axes[0].title, axes[0].xaxis.label, axes[0].yaxis.label] +\n",
    "             axes[0].get_xticklabels() + axes[0].get_yticklabels()\n",
    "             + [axes[1].title, axes[1].xaxis.label, axes[1].yaxis.label] +\n",
    "             axes[1].get_xticklabels() + axes[1].get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "axes[0].set_yticklabels([])\n",
    "axes[0].set_xticklabels([])\n",
    "axes[1].set_yticklabels([])\n",
    "axes[1].set_xticklabels([])\n",
    "\n",
    "for ax in axes:\n",
    "    # Get all the Line2D objects in the axis and change their linewidth\n",
    "    for line in ax.get_lines():\n",
    "        line.set_linewidth(3)\n",
    "\n",
    "print(d)\n",
    "print(f'Final Lag: {best_path[-1][0] - best_path[-1][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
